{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  .\\.pyspark\\Scripts\\activate  # enter this in cmd first to activate the virtual environment\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "spark_home = r\"D:\\senior 2 term 2\\bigdata\\releatedStuffforeEnviroment\\spark_unzipped\"\n",
    "os.environ[\"SPARK_HOME\"] = spark_home\n",
    "\n",
    "# Add Spark bin and executors to PATH\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(spark_home, \"bin\")\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(spark_home, \"sbin\")\n",
    "\n",
    "# Add Spark Python libraries to PYTHONPATH\n",
    "os.environ[\"PYTHONPATH\"] = os.path.join(spark_home, \"python\") + os.pathsep + os.environ.get(\"PYTHONPATH\", \"\")\n",
    "os.environ[\"PYTHONPATH\"] += os.pathsep + os.path.join(spark_home, \"python\", \"lib\")\n",
    "\n",
    "# Add PySpark to the system path\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(spark_home, \"python\", \"lib\", \"pyspark.zip\")\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(spark_home, \"python\", \"lib\", \"py4j-0.10.9-src.zip\")\n",
    "\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "# Test the setup\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark-Script\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"preprocessing_spark_output2.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import ast\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df_filtered = df.dropna(subset=['years_or_months_experience', 'orgTags_SKILLS'])\n",
    "df_filtered=df_filtered.filter(df_filtered.years_or_months_experience.contains('years'))\n",
    "df_filtered=df_filtered.filter(df_filtered.orgTags_SKILLS.contains(\"['\"))\n",
    "df_filtered=df_filtered.filter(df_filtered.orgTags_SKILLS.contains(\"0\")==False)\n",
    "df_filtered=df_filtered.filter(df_filtered.years_or_months_experience.contains('NOT FOUND')==False) \n",
    "df_filtered=df_filtered.select('orgTags_SKILLS','years_or_months_experience')\n",
    "df_filtered=df_filtered.filter(df_filtered.orgTags_SKILLS.contains('NOT FOUND')==False)\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "df_filtered = df_filtered.withColumn(\n",
    "    \"years_or_months_experience\",\n",
    "    regexp_replace(col(\"years_or_months_experience\"), r\"\\s*years\\s*\", \"\").cast(\"integer\")\n",
    ")\n",
    "df_filtered = df_filtered.filter(df_filtered.years_or_months_experience<10)\n",
    "\n",
    "df_filtered.printSchema()\n",
    "df_filtered.show()\n",
    "plt.boxplot(df_filtered.toPandas()['years_or_months_experience'])\n",
    "plt.show()\n",
    "#############################################################################################################\n",
    "df_pd = df_filtered.toPandas()\n",
    "\n",
    "# Apply ast.literal_eval correctly\n",
    "df_pd['orgTags_SKILLS'] = df_pd['orgTags_SKILLS'].apply(ast.literal_eval)\n",
    "#normalize the orgTags_SKILLS all small\n",
    "df_pd['orgTags_SKILLS'] = df_pd['orgTags_SKILLS'].apply(lambda x: [y.lower() for y in x])\n",
    "df_pd['orgTags_SKILLS'] = df_pd['orgTags_SKILLS'].apply(lambda x: [y.replace(\" \",\"\") for y in x])\n",
    "df_pd['orgTags_SKILLS'] = df_pd['orgTags_SKILLS'].apply(lambda x: [y.replace(\"(\",\"\") for y in x])\n",
    "df_pd['orgTags_SKILLS'] = df_pd['orgTags_SKILLS'].apply(lambda x: [y.replace(\")\",\"\") for y in x])\n",
    "\n",
    "df_pd.to_csv('yearsAndSkills.csv', index=False)    \n",
    "\n",
    "\n",
    "print(len(df_pd['orgTags_SKILLS']))\n",
    "print(len(df_pd['years_or_months_experience']))\n",
    "print(df_pd['orgTags_SKILLS'][0])\n",
    "# One hot encode the orgTags_SKILLS\n",
    "mlb = MultiLabelBinarizer()\n",
    "df_pd = df_pd.join(pd.DataFrame(mlb.fit_transform(df_pd.pop('orgTags_SKILLS')),\n",
    "                          columns=mlb.classes_,\n",
    "                          index=df_pd.index))\n",
    "df_pd.to_csv('testing.csv', index=False)\n",
    "model = LinearRegression()\n",
    "X = df_pd.drop('years_or_months_experience', axis=1)\n",
    "\n",
    "y = df_pd['years_or_months_experience']\n",
    "\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "print(mean_squared_error(y, y_pred))\n",
    "plt.scatter(y, y_pred)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.show()\n",
    "\n",
    "#input data to predict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(skill_list):\n",
    "    # Normalize and preprocess the new input in the same way as training data\n",
    "    skill_list = [skill.lower().replace(\" \", \"\").replace(\"(\", \"\").replace(\")\", \"\") for skill in skill_list]\n",
    "    \n",
    "    # Transform the skills using the same mlb instance\n",
    "    try:\n",
    "        transformed_skills = mlb.transform([skill_list])\n",
    "    except ValueError:\n",
    "        # Handle case where the skill is not recognized\n",
    "        print(\"Error: This skill was not seen in the training data and cannot be processed.\")\n",
    "        return None\n",
    "\n",
    "    # Create a DataFrame (simulate the same structure as during training)\n",
    "    input_df = pd.DataFrame(transformed_skills, columns=mlb.classes_)\n",
    "\n",
    "    # Predict using the trained model\n",
    "    prediction = model.predict(input_df)\n",
    "    prediction = max(0, prediction)\n",
    "\n",
    "    return prediction\n",
    "\n",
    "# Example usage:\n",
    "predicted_years = make_prediction(['figma, adobe xd'])\n",
    "if predicted_years is not None:\n",
    "    print(\"Predicted years of experience:\", predicted_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Combine X and y into a single DataFrame for correlation computation\n",
    "df_full = pd.concat([X, y], axis=1)\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = df_full.corr()\n",
    "\n",
    "# The target variable is 'years_of_months_experience', let's focus on its correlation with features\n",
    "target_corr = corr_matrix['years_or_months_experience'].sort_values(ascending=False)\n",
    "\n",
    "# Plotting the correlations of the most correlated features\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x=target_corr.index, y=target_corr.values)\n",
    "plt.xticks(rotation=90)  # Rotate labels for better readability\n",
    "plt.title('Feature Correlation with Years of Experience')\n",
    "plt.show()\n",
    "# Example: Focusing on top 10 most correlated features\n",
    "top_positive = target_corr.head(10).index  # Top 10 positive correlations\n",
    "top_negative = target_corr.tail(10).index  # Top 10 negative correlations\n",
    "top_features = top_positive.union(top_negative)  # Combine top positive and negative\n",
    "filtered_corr_matrix = df_full[top_features].corr()\n",
    "\n",
    "# Plotting the focused heatmap\n",
    "plt.figure(figsize=(8, 6))  # Smaller figure size\n",
    "sns.heatmap(filtered_corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', center=0)\n",
    "plt.title('Heatmap of Top 10 Correlated Features')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "df_revomJobna = df.dropna(subset=[\"position_name\"])\n",
    "job_names = df_revomJobna.select(\"position_name\")\n",
    "text_data = job_names.rdd.map(lambda row: row.position_name).collect()\n",
    "# Generate word cloud\n",
    "print(text_data)\n",
    "text_data = \",\".join(text_data)\n",
    "print(text_data[1:100])\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(text_data)\n",
    "\n",
    "# Display the cloud\n",
    "plt.figure(figsize=(100,80))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_revomJobna = df.dropna(subset=[\"orgAddress_state\"])\n",
    "job_state = df_revomJobna.select(\"orgAddress_state\")\n",
    "job_state.show(5)\n",
    "text_data = job_state.rdd.map(lambda row: row.orgAddress_state).collect()\n",
    "text_string = \" \".join(text_data)\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(text_string)\n",
    "\n",
    "# Display the cloud\n",
    "plt.figure(figsize=(100, 80))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "# Drop rows where 'years_or_months_experience' is null\n",
    "df_remove_null = df.dropna(subset=[\"years_or_months_experience\"])\n",
    "\n",
    "\n",
    "# Filter rows where 'years_or_months_experience' contains 'years'\n",
    "job_years = df_remove_null.filter(df_remove_null['years_or_months_experience'].contains('years'))\n",
    "job_years=job_years.filter(job_years['years_or_months_experience'].contains('=')==False)\n",
    "\n",
    "# Clean up the 'years_or_months_experience' column\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "job_years = job_years.withColumn(\n",
    "    \"years_experience\",\n",
    "    regexp_replace(col(\"years_or_months_experience\"), r\"\\s*years\\s*\", \"\").cast(\"integer\")\n",
    ")\n",
    "job_years = job_years.filter(job_years.years_experience.isNotNull())\n",
    "\n",
    "plt.boxplot(job_years.select(\"years_experience\").rdd.flatMap(lambda x: x).collect())\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "job_years=job_years.filter(job_years.years_experience<25)\n",
    "plt.boxplot(job_years.select(\"years_experience\").rdd.flatMap(lambda x: x).collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remove_null = df.dropna(subset=[\"orgTags_SKILLS\"])\n",
    "job_skills = df_remove_null.select(\"orgTags_SKILLS\")\n",
    "job_skills=job_skills.filter(job_skills['orgTags_SKILLS'].contains('NOT FOUND')==False)\n",
    "job_skills.show(5)\n",
    "job_skills = job_skills.rdd.map(lambda row: row.orgTags_SKILLS).collect()\n",
    "text_data = \",\".join(job_skills)\n",
    "text_data = text_data.replace(\"'\", \"\")\n",
    "text_data = text_data.replace('\"', \"\")\n",
    "print(text_data[1:100])\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(text_data)\n",
    "plt.figure(figsize=(100,80))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remove_null = df.dropna(subset=[\"orgTags_REQUIREMENTS\"])\n",
    "job_skills = df_remove_null.select(\"orgTags_REQUIREMENTS\")\n",
    "job_skills=job_skills.filter(job_skills['orgTags_REQUIREMENTS'].contains('NOT FOUND')==False)\n",
    "job_skills.show(5)\n",
    "job_names = job_skills.rdd.map(lambda row: row.orgTags_REQUIREMENTS).collect()\n",
    "text_data = \",\".join(job_names)\n",
    "text_data = text_data.replace(\"'\", \"\")\n",
    "text_data = text_data.replace('\"', \"\")\n",
    "print(text_data[1:100])\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(text_data)\n",
    "plt.figure(figsize=(100,80))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remove_null = df.dropna(subset=[\"todo\"])\n",
    "job_skills = df_remove_null.select(\"todo\")\n",
    "job_skills=job_skills.filter(job_skills['todo'].contains('NOT FOUND')==False)\n",
    "job_skills.show(5)\n",
    "job_names = job_skills.rdd.map(lambda row: row.todo).collect()\n",
    "text_data = \",\".join(job_names)\n",
    "text_data = text_data.replace(\"'\", \"\")\n",
    "text_data = text_data.replace('\"', \"\")\n",
    "print(text_data[1:100])\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(text_data)\n",
    "plt.figure(figsize=(100,80))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # Step 1: Create sample data\n",
    "# data = {\n",
    "#     'Job Title': ['Account Manager', 'Risk Manager', 'Construction Manager', 'Account Manager', 'Construction Manager'],\n",
    "#     'Years of Experience': [5, 7, 10, 3, 8]\n",
    "# }\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Step 2: Apply one-hot encoding to the 'Job Title' column\n",
    "# df_encoded = pd.get_dummies(df, columns=['Job Title'])\n",
    "\n",
    "# # Step 3: Normalize the 'Years of Experience' column\n",
    "# scaler = MinMaxScaler()\n",
    "# df_encoded['Years of Experience'] = scaler.fit_transform(df_encoded[['Years of Experience']])\n",
    "\n",
    "# # Step 4: Create the heat map\n",
    "# plt.figure(figsize=(10, 6))  # Set the figure size for better readability\n",
    "# sns.heatmap(df_encoded, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "# plt.title('Heat Map of Job Titles and Years of Experience')\n",
    "# plt.show()\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "df = spark.read.csv(\"preprocessing_spark_output2.csv\", header=True, inferSchema=True)\n",
    "df_revomJobna = df.dropna(subset=[\"position_name\"])\n",
    "job_names = df_revomJobna.select(\"position_name\")\n",
    "job_names.show(5)\n",
    "df_endcode_job = pd.get_dummies(job_names.toPandas())\n",
    "\n",
    "#will filter the years of experience form df_revomJobna\n",
    "\n",
    "df_remove_null = df_revomJobna.dropna(subset=[\"years_or_months_experience\"])\n",
    "\n",
    "\n",
    "# Filter rows where 'years_or_months_experience' contains 'years'\n",
    "job_years = df_remove_null.filter(df_remove_null['years_or_months_experience'].contains('years'))\n",
    "job_years=job_years.filter(job_years['years_or_months_experience'].contains('=')==False)\n",
    "\n",
    "# Clean up the 'years_or_months_experience' column\n",
    "job_years = job_years.withColumn(\n",
    "    \"years_experience\",\n",
    "    regexp_replace(col(\"years_or_months_experience\"), r\"\\s*years\\s*\", \"\").cast(\"integer\")\n",
    ")\n",
    "job_years = job_years.filter(job_years.years_experience.isNotNull())\n",
    "job_years=job_years.filter(job_years.years_experience<25)\n",
    "job_years_filtered =job_years.select(\"years_experience\")\n",
    "job_years_filtered.show(5)\n",
    "scaler = MinMaxScaler()\n",
    "job_years_filtered['years_experience'] = scaler.fit_transform(job_years_filtered[['years_experience']])\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size for better readability\n",
    "sns.heatmap(job_years_filtered.toPandas(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Heat Map of Job Titles and Years of Experience')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace\n",
    "\n",
    "\n",
    "# Load and preprocess data\n",
    "df = spark.read.csv(\"preprocessing_spark_output2.csv\", header=True, inferSchema=True)\n",
    "df = df.dropna(subset=[\"position_name\", \"years_or_months_experience\"])\n",
    "\n",
    "# Filter to include only 'years' in 'years_or_months_experience'\n",
    "job_years = df.filter(df['years_or_months_experience'].contains('years'))\n",
    "job_years = job_years.withColumn(\n",
    "    \"years_experience\",\n",
    "    regexp_replace(col(\"years_or_months_experience\"), r\"\\s*years\\s*\", \"\").cast(\"integer\")\n",
    ")\n",
    "job_years = job_years.filter(job_years['years_experience'] < 25)\n",
    "job_years = job_years.filter(job_years['years_experience'].isNotNull())\n",
    "job_years.show(5)\n",
    "\n",
    "# Convert to Pandas DataFrame for visualization\n",
    "pd_job_years = job_years.select(\"years_experience\").toPandas()\n",
    "job_years.show(5)\n",
    "\n",
    "# Normalize 'years_experience'\n",
    "scaler = MinMaxScaler()\n",
    "pd_job_years['years_experience'] = scaler.fit_transform(pd_job_years[['years_experience']])\n",
    "pd_job_years.head()\n",
    "\n",
    "\n",
    "# One-hot encoding for job titles\n",
    "pd_encoded_titles = pd.get_dummies(df.select(\"position_name\").toPandas(), columns=['position_name'])\n",
    "\n",
    "# Concatenate years of experience with one-hot encoded titles\n",
    "final_df = pd.concat([pd_job_years, pd_encoded_titles], axis=1)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))  # Adjust the size to fit all columns\n",
    "sns.heatmap(final_df.corr(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap of Job Titles and Years of Experience')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
